---
title: "Introduction to multiple testing"
author: "[Michael Love](http://mikelove.github.io)"
output: html_document
---

Here I will introduce a popular statistical framework for working with
the results of many parallel statistical tests, a setting that occurs
frequently in genomic analyses. I won't go into depth here, but will
discuss more in lecture. For background you can also read the
[lecture notes](http://genomicsclass.github.io/book/pages/multiple_testing.html)
from the HarvardX Biological Data Science series (also the Wikipedia
article on 
[multiple comparisons](https://en.wikipedia.org/wiki/Multiple_comparisons_problem) 
is a good introduction).

While up to this point, we have minimized the use of simulation data
to show concepts in Computational Biology, multiple testing is often
introduced using simulation data, and there is a practical reason
why. We can use real biological datasets to generate test
statistics from the null hypothesis, by taking a large group of
samples and randomly dividing into two groups with no known biological
difference. Unless we happen to choose a partition which corresponds
to biological or technical factors, this is pretty good for observing
null test statistics. And we can limit this probability by choosing a
large dataset, or by balancing our random partition with respect to
technical factors, or by removing technical factors using factor
analysis methods such as [SVA, RUV, or PEER](../dist/sva.html).

However, it's not easy to generate many test statistics from real data
which combine known null and alternative hypotheses, because we often
do not know which hypotheses are truly null, e.g. $\Delta = 0$, and
which are alternative, $\Delta \ne 0$. So we often resort to
simulation, or a combination of real null data with simulation used to
create alternative hypotheses, to demonstrate the concepts of multiple
test correction. Here we will use the latter, starting with a real
dataset where we can create a null distribution of test statistics.

# About the dataset

We start by downloading a large, normalized matrix of values for
*chromatin accessibilty* across 68 human donors. Chromatin
accessibility refers to a quantitative measurement of whether the DNA
at a certain location is packaged tightly or in a more open pattern,
which indicates the degree to which proteins can access and bind to
DNA -- in particular regulatory proteins which might affect
transcription of genes. A diagram of inaccessible and accessible
chromatin can be found on this 
[Wikipedia page](https://en.wikipedia.org/wiki/Chromatin_remodeling).

The dataset is hosted at
[this website](http://mitra.stanford.edu/kundaje/portal/chromovar3d/dnase.html),
and we download a text file of normalized data contained in this 
[subfolder](http://chromovar3d.stanford.edu/QTLs/correctedSignal/), 
which is about 130 Mb, compressed. The publication associated with
this dataset is listed at the above link, and the goal was to find
genetic variants which are associated with differences in chromatin
accessibility. We won't discuss here exactly how the raw data was
processed to produce the normalized matrix above, but for our
purposes, all we need to know is that large values correspond to more
accessible chromatin, each column corresponds to a different human
donor, and each row corresponds to a distinct location in the
genome. The rows are actually *peaks* of accessibility that were
detected in at least some subset of the samples.

```{r echo=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

We have to use this `skip=1` argument below because the first row
contains only column names, but is missing a column name for the first
column (the index number of the peak from a larger set of peaks).

```{r}
url <- "http://chromovar3d.stanford.edu/QTLs/correctedSignal/hMat.norm.ALL.dhs.peer_lS_5.txt.gz"
file <- "hMat.norm.ALL.dhs.peer_lS_5.txt.gz"
if (!file.exists(file)) download.file(url, file)
library(readr)
dnase <- read_delim(file, delim=" ", skip=1, col_names=FALSE)
dnase
```

```{r}
dim(dnase)
col.names <- names(read.delim(file, nrow=1, sep=" "))
col.names
```

We have 250k peaks and 68 donors. We add the column names that we
skipped before:

```{r}
mat <- as.matrix(dnase[,-1])
dim(mat)
colnames(mat) <- col.names
```

I take a quick look at the histogram of values for the first four peaks:

```{r}
par(mfrow=c(2,2))
for (i in 1:4) hist(mat[i,], col="grey")
```

Also, it's always a good idea to make a PCA plot of the samples, to
look for any large-scale structure in the data:

```{r}
pc <- prcomp(t(mat))
plot(pc$x[,1:2])
```

If you square the standard deviation values and sum them, you get the
total variance in the data. Here I show that the top PC is not a large
percent of the variance in the data, which means that there is not a
low dimensional structure to the data, which one might see with
strong batch effects or distinct biological conditions.

```{r}
plot(pc$sdev[1:10]^2 / sum(pc$sdev^2), type="b", ylab="% Var")
```

# Multiple testing

To investigate multiple testing, we first generate a mock comparison:
I choose two random groups of the data, indicated by a factor
variable called `fake` and perform row-wise t-tests across all the
250k peaks in the data. I then look at the distribution of p-values
across 20 bins defined on [0,1].

```{r}
set.seed(1)
fake <- factor(sample(rep(1:2,each=ncol(se)/2)))
ts <- rowttests(mat, fake)
nbins <- 20
brks <- 0:nbins/nbins
hist(ts$p.value, col="grey", breaks=brks)
```

Here I make this plot again, and show a line for what we would expect
on average for each bin for null hypotheses. We can see we are pretty
close to the expected values for this set of bins.

```{r}
p <- ts$p.value
hist(p, col="grey", breaks=brks)
abline(h=length(p)/nbins, col="dodgerblue", lwd=3)
1/nbins
sum(p < 1/nbins)
length(p)/nbins
```

Now, because we want to explore how multiple testing frameworks deal
with combined null and alternative hypotheses, I will "spike in"
10,000 p-values corresponding to alternative hypotheses. I don't
generate the data, just p-values which, after log10 transformation,
are uniformly distributed on [-6,-1].

```{r}
p2 <- p
p2[1:10000] <- 10^runif(10000,-6,-1)
```

# Correction with `p.adjust`

I demonstrate two types of correction for multiple testing, the
Bonferroni method and the Benjamini-Hochberg method, which are both
implemented in the `p.adjust` function in R. The `p.adjust` function
provides adjusted p-values, such that thresholding on the adjusted
p-values delivers a set obeying certain statistics properties. As we
went over in lecture, the Bonferroni method bounds the family-wise
error rate (FWER) while the Benjamini-Hochberg method bounds the false
discovery rate (FDR) in expectation. A reminder: these are critically
different bounds: FWER is the number of false positives over all null
hypotheses, while FDR is the number of false positives over the set
which is called positive. And the Benjamini-Hochberg method provides
the FDR bound in expectation.

Below we show the histogram of adjusted p-values, and the minimal
adjusted p-value. The adjustment is simply the original p-value
multiplied by the number of tests. So we could provide FWER control
for this particular simulated data for a few hundred tests, but only
at a high rate.

```{r}
padj.bonf <- p.adjust(p2, method="bonferroni")
hist(padj.bonf, col="grey", breaks=brks)
min(padj.bonf)
min(p2) * length(p2) == min(padj.bonf)
sum(padj.bonf < .5)
```

The false discovery rate is a much more practical and desirable bound
for genomic data analysis, compared to the family-wise error
rate. Investigators are often interested in the rate of false
positives among the set of most promising features (genes, or in this
case, genomic regions) that are identified by a statistical test.

Here, we pick up on thousands of tests where we can control the FDR at
5%, for example.

```{r}
padj.bh <- p.adjust(p2, method="BH")
hist(padj.bh, col="grey", breaks=brks)
min(padj.bh)
sum(padj.bh < .05)
```

One way to think about what our adjusted p-values are delivering, is
to examine a histogram of the original p-values for various bin sizes
along [0,1]. We see an enrichment of small p-values in the smallest
bin. If we draw the line of expected counts if all the hypotheses
were null, we see that the first bin could be expected to contain more
than half null hypotheses. This corresponds to the largest adjusted
p-value in this bin, which makes sense: by thresholding on an adjusted
p-value, we should obtain a set of hypotheses bounded by a given false 
discovery rate in expectation.

```{r}
nbins <- 20
brks <- 0:nbins/nbins
hist(p2, col="grey", breaks=brks)
abline(h=length(p)/nbins, col="dodgerblue", lwd=3)
max(padj.bh[p2 < 1/nbins])
```

If we make the bins smaller, the ratio of expected nulls decreases
relative to the height of the bar for the first bin, and again the
maximum adjusted p-value matches the expected proportion in that bin.

```{r}
nbins <- 100
brks <- 0:nbins/nbins
hist(p2, col="grey", breaks=brks)
abline(h=length(p)/nbins, col="dodgerblue", lwd=3)
max(padj.bh[p2 < 1/nbins])
```

# Runs of identical adjusted p-values

Finally, I want to show a property of adjusted p-values using the
Benjamini-Hochberg (BH) method, which sometimes surprises users who
aren't familiar with the procedure. See the runs of identical adjusted
p-values for the first 2000 sorted p-values:

```{r}
padj.sort <- sort(padj.bh)
plot(-log10(padj.sort[1:2000]), xlab="i", ylab="p")
plot(-log10(padj.sort[1:1200]), xlab="i", ylab="p")
```


The [publication of the BH method](http://www.jstor.org/stable/2346101)
describes a procedure, whereby one finds the largest *i* such that the
i-th smallest p-value is less than $\frac{i}{m} q$, where *q* is the
desired FDR bound. The adjusted p-values are the smallest value of *q*
for each test, so a *reverse* of the procedure where you start with
*q*. Geometrically, the BH procedure can be thought of putting the
tests in order of p-value along the x-axis, with the value of p on
the y-axis. For the first 2000 tests, this looks like:

```{r}
plot(1:2000, p.sort[1:2000], ylim=c(0,p.sort[2000]), type="l", xlab="i", ylab="p")
```

We then can find the smallest *q* to define a set, by drawing the line
$y(i) = \frac{q}{m} i$. So we draw a line that goes through the origin
with increasing slope. This first touches our sorted values of p at an
asymptotic point, and so all the p-values less than this point are
assigned the same adjusted p-value. They could only have a smaller
adjusted p-value if the line had touches the sorted p-values earlier.

```{r}
p.sort <- sort(p2)
q <- min(padj.bh)
i <- sum(padj.bh == q)
m <- length(p2)
plot(1:2000, p.sort[1:2000], ylim=c(0,p.sort[2000]), type="l")
abline(0, q/m, col="red")
```

# q-values

Finally, we mention that there is a method which improves upon the
BH method by attempting to estimate the proportion of
nulls in the set of all hypotheses, called $\pi_0 = \frac{m_0}{m}$. The
BH method controls the FDR in expectation for any $m_0 \le m$,
but if we can come up with a good estimate of $\pi_0$ when it is
less than 1, we can reject more hypotheses and still control the FDR
in expectation. Without getting into details, the `qvalue` function in
the qvalue package does this estimation, and provides q-values which
operate analogously to the adjusted p-values we defined above: using a
threshold on q-values produces a set with FDR which should be bounded
by that value in expectation. We can see that for this simulated data,
the estimate of $\pi_0$ is close to the truth:

```{r}
library(qvalue)
res <- qvalue(p2)
res$pi0
1 - 10000/length(p) # we simulated 10k alternative
```

The q-values are multiplicatively scaled relative to the BH adjusted
p-values, and are strictly smaller, although here only slightly
so. For smaller $\pi_0$ the gain in sensitivity would be more
substantial. 

```{r}
qval.sort <- sort(res$qvalues)
plot(padj.sort[1:2000], qval.sort[1:2000])
abline(0,1)
```
