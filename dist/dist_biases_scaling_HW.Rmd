---
title: "Homework 3 - Distances, biases, and scaling"
author: "your name here"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output: 
  html_document:
    fig_height: 7
---

# Question 1 - Regression bias: ignoring batch effect

Suppose you want to estimate the effect of treatment on gene
expression, for $n$ samples. Suppose $x_1$ tells us about the amount
of some continuous treatment, and suppose it is scaled to have unit
variance $\text{Var}(x_1) = 1$. You are provided some
pre-processed/scaled/appropriately transformed gene expression data
$y$ for some gene $G$. If there were no structural biases (sample
correlations) you could use the model (now omitting sample index $i$)

$$
y = b_0 + x_1 b_1 + e,
$$

where $b_0$ represents baseline expression (untreated samples), $b_1$
represents the treatment effect associated with $x_1$, and
$e \sim N(0, \sigma^2)$ represents the noise term.

However, suppose now there *is* a batch effect, where gene expression
measurements are shifted for technical reasons in a per sample manner
by an amount $x_2 b_2$ giving:

$$
y = b_0 + x_1 b_1 + x_2 b_2 + e.
$$

Suppose a continuous valued $x_2$ indicating degrees of batch
variation, with $\text{Var}(x_2) = 1$. Suppose that there is
confounding of the treatment assignment and the batch to some degree,
e.g. $x_1$ and $x_2$ are correlated with
$\text{cor}(x_1, x_2) = \rho$.

Suppose you are asked to estimate $b_1$ using linear regression of $y$
on $x_1$ only (so ignoring $x_2$). You can call this the Misspecified
Model, or MM. Compute the bias of the linear regression estimator
$\widehat{b}_1^{MM}$ for $b_1$, that is
$E\left(\widehat{b}_1^{MM}\right) - b_1$.

**Hint:** You can use the fact that, since $x_1$ and $x_2$ have unit
variance and have correlation $\rho$, $x_2$ can be written as: $$
x_2 = \rho x_1 + \sqrt{1 - \rho^2} z
$$ with $z \perp \!\!\! \perp x_1$ ($z$ independent of $x_1$).

# Question 2 - Median ratio vs column sum part I

We discussed in class one method to estimate a scalar factor to
account for variable *sequencing depth* (remember, the column sums
being very different across samples). That method was to estimate the
median of ratios of a sample compared to a pseudo-reference sample:
the geometric mean applied to each gene. Here you will compare this
estimator to the simple column sum. While the data and papers here
discuss RNA-seq, the concepts are useful generally for thinking about
scaling/normalization of many types of quantitative sequencing assays
(experiments).

Many have written about the problems with using the column sum to
scale genomic data. Two references are here:

[A scaling normalization method for differential expression analysis of RNA-seq data](https://genomebiology.biomedcentral.com/articles/10.1186/gb-2010-11-3-r25)

> Using TMM normalization in a statistical test for DE (see Materials
> and methods) results in a similar number of genes significantly
> higher in liver (47%) and kidney (53%). By contrast, the standard
> normalization (to the **total number of reads** as originally used in
> [6]) results in the majority of DE genes being significantly higher
> in kidney (77%). Notably, less than 70% of the genes identified as
> DE using standard normalization are still detected after TMM
> normalization (Table 1). In addition, we find the log-fold-changes
> for a large set of housekeeping genes (from [16]) are, on average,
> offset from zero very close to the estimated TMM factor, thus giving
> credibility to our robust estimation procedure. Furthermore, using
> the non-adjusted testing procedure, 8% and 70% of the housekeeping
> genes are significantly up-regulated in liver and kidney,
> respectively. After TMM adjustment, the proportion of DE
> housekeeping genes changes to 26% and 41%, respectively, which is a
> lower total number and more symmetric between the two tissues. 

[Evaluation of statistical methods for normalization and differential expression in mRNA-Seq experiments](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2838869/)

> The simplest form of normalization is achieved by scaling gene
> counts, in lane i, by a single lane-specific factor di. In essence,
> these global scaling factors define the null hypothesis of no
> differential expression: if a gene has the same proportions of
> counts across lanes as the proportions determined by the vector of
> di's, then it is deemed non-differentially expressed. The standard
> **total-count normalization** results in low variation across lanes,
> flow-cells, and library preparations, as discussed above. What has
> not been understood previously, is that this normalization technique
> reflects the behavior of a relatively small number of high-count
> genes: 5% of the genes account for approximately 50% of the total
> counts in both Brain and UHR. These genes are not guaranteed to have
> similar levels of expression across different biological conditions
> and, in the case of the MAQC-2 dataset, they are noticeably
> over-expressed in Brain, as compared to the majority of the genes
> (Figure 5). 

Install the `parathyroidSE` package from
Bioconductor, which contains RNA-seq data of cultured parathyroid
samples from four donors, subjected to a variety of treatments.

```{r message=FALSE}
library(parathyroidSE)
data(parathyroidGenesSE)
se <- parathyroidGenesSE
# the experimental design (not used yet):
as.data.frame(colData(se)[,3:5])
# filter out zero count genes:
se <- se[rowSums(assay(se)) > 0,]
```

You should recreate a plot similar to
[Figure 5a](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2838869/figure/F5/) 
from the second paper above, which plots the percent of total count
over the percent of genes contributing to the total count.
(Note the data are different so the curves will not be identical to
that figure.)

To do so, calculate the row sum of the (unscaled) counts across all genes in
the dataset. Sort the row sum from the gene with the highest counts to
the smallest counts (`decreasing=TRUE`). Then calculate the cumulative
sum of the sorted row sums. There is a function in R for calculating
the cumulative sum (try `cumsum(5:1)`).

Plot the contribution of the first 5%, 10%, 15%, 20% of genes 
(from highest count to lowest) towards the total count.

Roughly what percent of the total count is from the top 10% of genes? 

# Question 3 - Median ratio vs columns sum part II

We will examine what happens when the genes with the highest count are
potentially differentially expressed across condition, as mentioned by
the second paper above. How does this affect a robust scaling factor
(median ratio) and how does it affect a simple scaling factor (column
sums)?

We will do so by artificially manipulating the counts in the
parathyroid dataset such that the top 20 genes by count are doubled,
and we resave the new dataset as `se2`:

```{r}
se2 <- se # make a copy
o <- order(rowSums(assay(se)), decreasing=TRUE)[1:20]
cts <- 2 * assay(se)[o, se$treatment == "DPN"]
assay(se2)[o, se$treatment == "DPN"] <- cts
```

Construct two MA plots to see the effect of this manipulation. The
MA plots should be as we saw in class `y - x` on the vertical axis, and 
`0.5 (x + y)` on the horizontal axis, where `x` and `y` are both on
the log scale. `x` should be the mean of the log scale expression
(counts) for the control samples, and `y` should be the mean of the
log scale expression (counts) for the DPN samples. 

For the original dataset, this looks like:

```{r}
library(DESeq2)
# using the original dataset, 'se':
dds <- DESeqDataSet(se, design=~1) # ignore the design...
dds <- estimateSizeFactors(dds) # median ratio scaling
ntd <- normTransform(dds) # log2 of scaled data
```

Note that you can pull out the log2 scaled data with `assay`. E.g. the
matrix of log2 scale expression values for the control samples:

```{r}
ctrl.mat <- assay(ntd)[,ntd$treatment == "Control"]
```

Note that you can provide alternative scaling factors before running
`normTransform` with the setter function, `sizeFactors(x) <-`.
It's a good idea to divide out the geometric mean of any scaling
factors, so they are centered around 1. Here's an example with dummy
scaling factors:

```{r eval=FALSE}
mySF <- abs(rnorm(27, mean=0, sd=10)) # dummy values for scaling factors
mySF <- mySF / exp(mean(log(mySF))) # divide out geo mean
sizeFactors(dds) <- mySF # set the 'size factors' to use for scaling
ntd <- normTransform(dds) # log2 of scaled data, using custom scaling factors
```

Make an MA plot for `se2` using median ratio scaling (default) and
alternatively using the column sum to compute scaling factors. Comment
on the qualitative difference, on datasets with large differences
affecting the most highly expressed genes.

# Question 4 - derive VST for two distributions

An approximate variance stabilizing transformation is one which, after
applying to the data provides roughly flat variance (across samples)
regardless of the mean value of the feature (across samples). There is
a formula which provides the VST. First let us define a variance mean
relationship as follows, for feature $i$:

$$ \sigma^2_i = g(\mu_i) $$

We will seek a VST $f(.)$ such that $Var(f(Y_i))$ is approximately
constant.

Consider the first order Taylor series approximation of $f(Y_i)$:

$$ f(Y_i) \approx f(\mu_i) + (Y_i - \mu_i) f'(\mu_i) $$
then

$$
\begin{align}
Var(f(Y_i)) &\approx [f'(\mu_i)]^2 Var[(Y_i - \mu_i)] \\
&\approx [f'(\mu_i)]^2 \sigma^2_i
\end{align}
$$

So we seek $f(.)$ such that:

$$ [f'(\mu_i)]^2 = 1/g(\mu_i) $$

By re-arranging terms and integrating with respect to $\mu_i$ we have:

$$ f(\mu) = \int \frac{d\mu}{\sqrt{g(\mu)}} $$ 

Derive VST for the following two datasets. The first one is Poisson
with $Var(Y_i) = \mu_i$ and the second is log-Normal with 
$Var(Y_i) \propto \mu_i^2$.

```{r}
library(vsn)
mu <- seq(from=1,to=100,length=2000)
mat <- matrix(rpois(10*length(mu), mu), ncol=10)
meanSdPlot(mat, ranks=FALSE)
```

```{r}
mu2 <- log(seq(from=1,to=100,length=2000))
mat2 <- matrix(exp(rnorm(10*length(mu2), mu2)), ncol=10)
meanSdPlot(mat2, ranks=FALSE)
```

# Bonus Question - empirical bias estimates on gene expression

I've prepared a dataset with \~50 samples that are pre-treatment with
nivolumab and \~50 samples that are post-treatment. Ignore for this
problem that the samples may be paired, and just treat the two groups
as independent.

I have prepared scaled and transformed data for this question over 40
genes.

Suppose first you are only given the `condition` variable, and perform
linear regression-based testing to obtain an effect estimate and
p-value for each gene testing the null hypothesis of no change in
expression across treatment groups.

Next use the 1st batch factor `W_1` as a covariate in the model along
with `condition`, to control for batch.

Which is more common: genes becoming more significant (smaller $p$) after
controlling for batch with `W1`, or genes becoming less significant?

Estimate the bias for each gene in the first model, supposing that the second
model with $W_1$ is the correct model. Does this make sense given the
correlation of the batch variable and the condition?

Hint: you can use `limma::lmFit` for linear model computation. This
line of code gives standard t-statistics from `lmFit`:

`fit$coefficients / fit$stdev.unscaled / fit$sigma`

Hint #2: When $x_1$ and $x_2$ do not have unit variance, the result we
used before changes to:

$$
x_2 = \rho \frac{\sigma_{x_2}}{\sigma_{x_1}} x_1 + \sqrt{1 - \rho^2} z
$$

```{r}
load("preNivolumabOnNivolumab.rda")
se <- preNivolumabOnNivolumab
head(assay(se)[,1:5]) # scaled and transformed expression
colData(se)
table(se$condition)
se$condition <- factor(se$condition, c("Pre","On"))
head(se$W_1) # 1st batch factor
```
